<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><![endif]-->
<title>Yoni Kasten's Homepage</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<meta name="description" content="A PhD student at the Weizmann institute of science, supervised by Prof. Ronen Basri." />
<meta name="author" content="Yoni Kasten" />

<!-- favicons -->
<!-- <link rel="shortcut icon" href="images/templatemo_favicon.ico"> -->
<!-- bootstrap core CSS -->
<link href="js/bootstrap.css" rel="stylesheet">
<!-- fancybox CSS -->
<link href="js/jquery.css" rel="stylesheet">
<!-- flex slider CSS -->
<link href="js/flexslider.css" rel="stylesheet">
<!-- custom styles for this template -->
<link href="js/templatemo_style.css" rel="stylesheet">
<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
<![endif]-->
<style>
#cf3 {
align:left
position:relative;
width:240px;
margin:0 auto;
}

#cf3 img {
position:absolute;
left:0;
-webkit-transition: opacity 0.1s ease-in-out;
-moz-transition: opacity 0.1s ease-in-out;
-o-transition: opacity 0.1s ease-in-out;
transition: opacity 0.1s ease-in-out;
}

@keyframes cf3FadeInOut {
0% {
opacity:1;
}
45% {
opacity:1;
}
55% {
opacity:0;
}
100% {
opacity:0;
}
}

#cf3 img.top {
animation-name: cf3FadeInOut;
animation-timing-function: ease-in-out;
animation-iteration-count: infinite;
animation-duration: 1s;
animation-direction: alternate;
}

</style>

</head>
<body>
<header>
    <div class="container">
        <div class="row">
           
            <div class="col-md-3 hidden-xs"></div>
            <div class="col-xs-3 col-xs-offset-20 visible-xs">
                <a href="#" id="mobile_menu"><span class="glyphicon glyphicon-align-justify"></span></a>
            </div>
            <div class="col-xs-24 visible-xs" id="mobile_menu_list">
                <ul style="display: none;">
<li><a href="#templatemo_about" class="current">About</a></li>
                    <!-- <li><a href="#templatemo_slideshow">Slideshow</a></li> -->
                    <li><a href="#templatemo_publications">Publications</a></li>
                </ul>
            </div>
            <div class="col-md-16 col-sm-18 hidden-xs" id="templatemo-nav-bar">
                <ul class="nav navbar-right">
<li><a href="#templatemo_about" class="current">About</a></li>
                    <!-- <li><a href="#templatemo_slideshow">Slideshow</a></li> -->
                    <li><a href="#templatemo_publications">Publications</a></li>
                </ul>
            </div>
        </div>
    </div>
</header><!-- end of templatemo_header -->

<section id="templatemo_about">
    <div class="container">
        <div class="row">
            <div class="col-md-2"></div>
            <div id="my_photo" class="col-md-5 col-sm-7 col-xs-24">
                <img src="images/profilenew.jpg" alt="image 1">
            </div>
            <div class="col-md-1"></div>
            <div class="col-md-16">
                <h2>Yoni Kasten</h2>
<p>
I am a Research Scientist at NVIDIA Research. I'm interested in Computer Vision and Machine Learning. My research is mostly in the domain of 3D computer vision (e.g. Camera Localization, Structure From Motion and 3D reconstruction) and has recently focused on deep neural models for computer vision problems that involve geometry. I recently completed my PhD at Weizmann Institute of Science
under the supervision of <a href="http://www.weizmann.ac.il/math/ronen/home/" target="_blank">Prof. Ronen Basri</a> from the Department of Computer Science and Applied Mathematics at the Weizmann Institute of Science.
I did my B.Sc. in Electrical Engineering at the Hebrew University of Jerusalem, where I also did my M.Sc. in Computer Science under the supervision of <a href="https://www.cs.huji.ac.il/~peleg/" target="_blank">Prof. Shmuel Peleg</a> and <a href="https://www.cse.huji.ac.il/~werman/" target="_blank">Prof. Michael Werman. </a>
<font size=1></font></br>
<b>Email:</b> yonikasten <font color="grey">at</font> gmail <font color="grey">dot</font> com</br>
</p>

  <p>
                <h4>Teaching</h4>
                <ul>
                    <li>2021/spring (WIS):  Multiple View Geometry for Computer Vision Applications (lecturer) </li>
                    <li>2020/spring (WIS):  Multiple View Geometry for Computer Vision Applications (lecturer) </li>
                <li>2018/winter (WIS):  Introduction to Computer Vision (TA) </li>
				<li>2016/winter (HUJI):  Image Processing (TA) </li>
				<li>2015/winter (HUJI):  Image Processing (TA) </li>
                </ul>
                </p>
            </div>
        </div><!-- end of row -->
    </div>
</section><!-- end of templatemo_about -->




<section id="templatemo_publications">

 <div class="container">
<hr>
        <div class="row">
            <h1>Publications</h1>
        </div>


       <div class="row" id="templatemo_publications_LargeScaleBD">
             <div class="col-md-1"></div>
             <div class="col-md-5 col-sm-7 col-xs-24">
                 <img src="images/teaser_lucia.gif"  alt="">

             </div>
             <div class="col-md-1"></div>
             <div class="col-md-16">
                 <h2>Layered Neural Atlases for Consistent Video Editing</h2>
                 <p>
                   Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel<br>
                   <i> SIGGRAPH Asia 2021 </i>
                 </p>
                 <a class="btn btn-default abstract" ptitle="We present a method that decomposes, and “unwraps”, an input video into a set of layered 2D atlases, each providing a unified representation of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to a single 2D atlas (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image. "> Abstract</a>
                 <a href="https://arxiv.org/pdf/2109.11418.pdf" class="btn btn-default" target="_blank">Paper </a>
                 <a href="https://layered-neural-atlases.github.io/" class="btn btn-default" target="_blank">Project page </a>
                    <a href="https://github.com/ykasten/layered-neural-atlases" class="btn btn-default" target="_blank">Code </a>

             </div>

      </div><!-- end of row -->



  <div class="row" id="templatemo_publications_LargeScaleBD">
             <div class="col-md-1"></div>
             <div class="col-md-5 col-sm-7 col-xs-24">
                 <img src="images/volsdf.gif"  alt="">

             </div>
             <div class="col-md-1"></div>
             <div class="col-md-16">
                 <h2>Volume Rendering of Neural Implicit Surfaces</h2>
                 <p>
                   Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman<br>
                   <i> Neural Information Processing Systems 2021 (NeurIPS'21)  </i> <br>
                   <p style="color:red;"> Oral presentation </p>
                 </p>
                 <a class="btn btn-default abstract" ptitle="Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace’s cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two."> Abstract</a>
                 <a href="https://arxiv.org/pdf/2106.12052.pdf" class="btn btn-default" target="_blank">Paper </a>
             </div>

      </div><!-- end of row -->

  <div class="row" id="templatemo_publications_LargeScaleBD">
             <div class="col-md-1"></div>
             <div class="col-md-5 col-sm-7 col-xs-24">
                 <img src="images/iccv2021.png"  alt=""><br>
                 <img src="images/four_optimization_results.gif"  alt="">

             </div>
             <div class="col-md-1"></div>
             <div class="col-md-16">
                 <h2>Deep Permutation Equivariant Structure from Motion</h2>
                 <p>
                   Dror Moran*, Hodaya Koslowsky*, Yoni Kasten, Haggai Maron, Meirav Galun, Ronen Basri (*equal contribution)<br>
                   <i> International Conference on Computer Vision 2021 (ICCV'21)   </i> <br>
                   <p style="color:red;"> Oral presentation</p>
                 </p>
                 <a class="btn btn-default abstract" ptitle="Existing deep methods produce highly accurate 3D reconstructions in stereo and multiview stereo settings, i.e., when cameras are both internally and externally calibrated. Nevertheless, the challenge of simultaneous recovery of camera poses and 3D scene structure in multiview settings with deep networks is still outstanding. Inspired by projective factorization for Structure from Motion (SFM) and by deep matrix completion techniques, we propose a neural network architecture that, given a set of point tracks in multiple images of a static scene, recovers both the camera parameters and a (sparse) scene structure by minimizing an unsupervised reprojection loss. Our network architecture is designed to respect the structure of the problem: the sought output is equivariant to permutations of both cameras and scene points. Notably, our method does not require initialization of camera parameters or 3D point locations. We test our architecture in two setups: (1) single scene reconstruction and (2) learning from multiple scenes. Our experiments, conducted on a variety of datasets in both internally calibrated and uncalibrated settings, indicate that our method accurately recovers pose and structure, on par with classical state of the art methods. Additionally, we show that a pre-trained network can be used to reconstruct novel scenes using inexpensive fine-tuning with no loss of accuracy."> Abstract</a>
                 <a href="https://arxiv.org/pdf/2104.06703.pdf" class="btn btn-default" target="_blank">Paper </a><a href="https://github.com/drormoran/Equivariant-SFM" class="btn btn-default" target="_blank">Code</a>
             </div>

      </div><!-- end of row -->



      <div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/hybrid.png" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>A hybrid global structure from motion method for synchronously
estimating global rotations and global translations</h2>
<p>Xin Wang, Teng Xiao, Yoni Kasten<p>
<tab1>  </tab1>
<p><i>ISPRS Journal of Photogrammetry and Remote Sensing 2021</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="A hybrid global structure from motion method for synchronously estimating global rotations and global translations"
abstract="Over the last few decades, the methods of global image orientation, which is also called global SfM, have attracted a lot of attention from researchers, mainly thanks to its advantage of time efficiency. Based on the input of relative orientation results, most conventional global SfM methods employ a two-step strategy consisting of global rotation estimation and global translation estimation. This paper, on the contrary, introduces a hybrid global approach that intends to solve global rotations and translations synchronously, but hierarchically. To improve the robustness and time efficiency, we first propose a novel efficient method that is much faster than the previous approaches for extracting an optimal minimum cover of a connected image triplet set (OMCTS). The OMCTS makes all the available images contained in a minimum number of connected image triplets, as well as all of those selected triplets, satisfy the constraint that the three corresponding relative orientations are as compatible as possible to each other. In order to solve non-collinear triplets in the OMCTS, some fundamental characterizations of essential matrices in the multiple-image setting are used, and image pose parameters are then estimated via averaging the constrained essential matrices. For the collinear triplets, the above approach is invalid and the image pose parameters are then alternatively determined from the relative orientations using the depth of tie points from each individual local spatial intersection. Finally, all image orientations are moved to a common coordinate system by traversing the solved connected triplets using similarity transformations. Compared to the state-of-the-art global SfM methods, the performance and capability of the proposed hybrid approach are thoroughly demonstrated on various public datasets (mainly including ordered and unordered internet images, oblique aerial images, hard and complex datasets, etc.). ">Abstract</a>
<a href="https://www.researchgate.net/profile/Xin-Wang-305/publication/349394725_A_hybrid_global_structure_from_motion_method_for_synchronously_estimating_global_rotations_and_global_translations/links/602e23ba4585158939b080c9/A-hybrid-global-structure-from-motion-method-for-synchronously-estimating-global-rotations-and-global-translations.pdf" target="_blank" class="btn btn-default abstract" ptitle="A hybrid global structure from motion method for synchronously estimating global rotations and global translations">Paper</a>

<!--<a href="https://github.com/YuvalBahat/Confidence_From_Invariance" class="btn btn-default">Code</a>-->
</div>
        </div>



  <div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/skull.gif" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance</h2>
<p>Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman<p>
<tab1>  </tab1>
<p><i>Neural Information Processing Systems 2020 (NeurIPS'20)</i></p>
</p>
     <p style="color:red;"> Spotlight presentation</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance"
abstract="In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.">Abstract</a>
<a href="https://arxiv.org/pdf/2003.09852.pdf" target="_blank" class="btn btn-default abstract" ptitle="Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance">Paper</a>
<a href="https://lioryariv.github.io/idr/" target="_blank" class="btn btn-default abstract" ptitle="Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance">Project Page</a>
<a href="https://github.com/lioryariv/idr" target="_blank" class="btn btn-default abstract" ptitle="Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance">Code</a>


<!--<a href="https://github.com/YuvalBahat/Confidence_From_Invariance" class="btn btn-default">Code</a>-->
</div>
        </div>
<div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/laplace.png" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>On the Similarity between the Laplace and Neural Tangent Kernels</h2>
<p>Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, Ronen Basri<p>
<tab1>  </tab1>
<p><i>Neural Information Processing Systems 2020 (NeurIPS'20)</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="On the Similarity between the Laplace and Neural Tangent Kernels"
abstract="Recent theoretical work has shown that massively overparameterized neural networks are equivalent to kernel regressors that use Neural Tangent Kernels(NTK). Experiments show that these kernel methods perform similarly to real neural networks. Here we show that NTK for fully connected networks is closely related to the standard Laplace kernel. We show theoretically that for normalized data on the hypersphere both kernels have the same eigenfunctions and their eigenvalues decay polynomially at the same rate, implying that their Reproducing Kernel Hilbert Spaces (RKHS) include the same sets of functions. This means that both kernels give rise to classes of functions with the same smoothness properties. The two kernels differ for data off the hypersphere, but experiments indicate that when data is properly normalized these differences are not significant. Finally, we provide experiments on real data comparing NTK and the Laplace kernel, along with a larger class of{\gamma}-exponential kernels. We show that these perform almost identically. Our results suggest that much insight about neural networks can be obtained from analysis of the well-known Laplace kernel, which has a simple closed-form.">Abstract</a>
<a href="https://arxiv.org/pdf/2007.01580.pdf" target="_blank" class="btn btn-default abstract" ptitle="On the Similarity between the Laplace and Neural Tangent Kernels">Paper</a>

<!--<a href="https://github.com/YuvalBahat/Confidence_From_Invariance" class="btn btn-default">Code</a>-->
</div>
        </div>


       <div class="row" id="templatemo_publications_LargeScaleBD">
             <div class="col-md-1"></div>
             <div class="col-md-5 col-sm-7 col-xs-24">
                 <img src="images/miccaiw.png"  alt="">

             </div>
             <div class="col-md-1"></div>
             <div class="col-md-16">
                 <h2>End-To-End Convolutional Neural Network for 3D Reconstruction of Knee Bones From Bi-Planar X-Ray Images</h2>
                 <p>
                   Yoni Kasten*, Daniel Doktofsky*, Ilya Kovler* (*equal contribution)<br>
                   <i> International Workshop on Machine Learning for Medical Image Reconstruction 2020  </i> <br>
                 </p>
                 <a class="btn btn-default abstract" ptitle="We present an end-to-end Convolutional Neural Network (CNN) approach for 3D reconstruction of knee bones directly from two bi-planar X-ray images. Clinically, capturing the 3D models of the bones is crucial for surgical planning, implant fitting, and postoperative evaluation. X-ray imaging significantly reduces the exposure of patients to ionizing radiation compared to Computer Tomography (CT) imaging, and is much more common and inexpensive compared to Magnetic Resonance Imaging (MRI) scanners. However, retrieving 3D models from such 2D scans is extremely challenging. In contrast to the common approach of statistically modeling the shape of each bone, our deep network learns the distribution of the bones’ shapes directly from the training images. We train our model with both supervised and unsupervised losses using Digitally Reconstructed Radiograph (DRR) images generated from CT scans. To apply our model to X-Ray data, we use style transfer to transform between X-Ray and DRR modalities. As a result, at test time, without further optimization, our solution directly outputs a 3D reconstruction from a pair of bi-planar X-ray images, while preserving geometric constraints. Our results indicate that our deep learning model is very efficient, generalizes well and produces high quality reconstructions."> Abstract</a>
                 <a href="https://arxiv.org/pdf/2004.00871.pdf" class="btn btn-default" target="_blank">Paper </a>
             </div>

      </div><!-- end of row -->



  <div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/icml2020.png" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>Frequency Bias in Neural Networks for Input of Non-Uniform Density</h2>
<p>Ronen Basri, Meirav Galun, Amnon Geifman,  David Jacobs, Yoni Kasten, Shira Kritchman (alphabetical order)<p>
<tab1>  </tab1>
<p><i>International Conference on Machine Learning 2020 (ICML'20)</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="Frequency Bias in Neural Networks for Input of Non-Uniform Density"
abstract="Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias -- networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency ?, convergence at a point $\x \in \Sphere^{d-1}$ occurs in time $O(\kappa^d/p(\x))$ where $p(\x)$ denotes the local density at $\x$. Specifically, for data in $\Sphere^1$ we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.">Abstract</a>
<a href="https://arxiv.org/pdf/2003.04560.pdf" target="_blank" class="btn btn-default abstract" ptitle="Frequency Bias in Neural Networks for Input of Non-Uniform Density">Paper</a>
<!--<a href="https://github.com/YuvalBahat/Confidence_From_Invariance" class="btn btn-default">Code</a>-->
</div>
        </div>


    <div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/cvpr2020.png" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>Averaging Essential and Fundamental Matrices in Collinear Camera Settings</h2>
<p>Amnon Geifman*, Yoni Kasten*, Meirav Galun  and Ronen Basri (*equal contribution)<p>
<tab1>  </tab1>
<p><i>Computer Vision and Pattern Recognition (CVPR'20), Seattle 2020</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="Averaging Essential and Fundamental Matrices in Collinear Camera Settings"
abstract="Global methods to Structure from Motion have gained popularity in recent years. A significant drawback of global methods is their sensitivity to collinear camera settings. In this paper, we introduce an analysis and algorithms for averaging bifocal tensors (essential or fundamental matrices) when either subsets or all of the camera centers are collinear.
 We provide a complete spectral characterization of bifocal tensors in collinear scenarios and further propose two averaging algorithms. The first algorithm uses rank constrained minimization to recover camera matrices in fully collinear settings. The second algorithm enriches the set of possibly mixed collinear and non-collinear cameras with additional, ''virtual cameras'', which are placed in general position, enabling the application of existing averaging methods to the enriched set of bifocal tensors. Our algorithms are shown to achieve state of the art results on various benchmarks that include autonomous car datasets and unordered image collections  in both calibrated and unclibrated settings.">Abstract</a>
<a href="https://arxiv.org/pdf/1912.00254.pdf" target="_blank" class="btn btn-default abstract" ptitle="Averaging Essential and Fundamental Matrices in Collinear Camera Settings">Paper</a>
<a href="https://www.youtube.com/watch?v=P1stdNCttZY" class="btn btn-default" target="_blank">Video Lecture (Israel Computer Vision Day 2019) </a>
<!--<a href="https://github.com/YuvalBahat/Confidence_From_Invariance" class="btn btn-default">Code</a>-->
</div>
        </div>

<div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/nips2019.PNG" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>The Convergence Rate of Neural Networks for</h2>
<h2>Learned Functions of Different Frequencies</h2>
<p>Ronen Basri, David Jacobs, Yoni Kasten and Shira Kritchman (alphabetical order)<p>
<tab1>  </tab1>
<p><i>Neural Information Processing Systems (NeurIPS'19), Vancouver 2019</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="The Convergence Rate of Neural Networks for
Learned Functions of Different Frequencies"
abstract="We study the relationship between the speed at which a neural network learns a
function and the frequency of the function. We build on recent results that show
that the dynamics of overparameterized neural networks trained with gradient descent can be well approximated by a linear system. When normalized training
data is uniformly distributed on a hypersphere, the eigenfunctions of this linear
system are spherical harmonic functions. We derive the corresponding eigenvalues for each frequency after introducing a bias term in the model. This bias term
had been omitted from the linear network model without significantly affecting
previous theoretical results. However, we show theoretically and experimentally
that a shallow neural network without bias cannot learn simple, low frequency
functions with odd frequencies, in the limit of large amounts of data. Our results
enable us to make specific predictions of the time it will take a network with bias
to learn functions of varying frequency. These predictions match the behavior of
real shallow and deep networks.
">Abstract</a>
<a href="https://arxiv.org/pdf/1906.00425.pdf" target="_blank" class="btn btn-default abstract" ptitle="The Convergence Rate of Neural Networks for
Learned Functions of Different Frequencies">Paper</a>
<a href="https://github.com/ykasten/Convergence-Rate-NN-Different-Frequencies" class="btn btn-default" target="_blank">Code</a>
<!--<a href="https://github.com/YuvalBahat/Confidence_From_Invariance" class="btn btn-default">Code</a>-->
</div>
        </div>


<div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/iccv2019.PNG" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>Algebraic Characterization of Essential Matrices </h2>
<h2>and Their Averaging in Multiview Settings</h2>
<p>Yoni Kasten*, Amnon Geifman*, Meirav Galun and  Ronen Basri  (*equal contribution)<p>
<tab1>  </tab1>
<p><i>International Conference on Computer Vision (ICCV'19), Seoul 2019</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="Algebraic Characterization of Essential Matrices and Their Averaging
in Multiview Settings"
abstract="Essential matrix averaging, i.e., the task of recovering camera locations and orientations in calibrated, multiview settings, is a first step in global approaches to Euclidean structure from motion. A common approach to essential matrix averaging is to separately solve for camera orientations and subsequently for camera positions. This paper presents a novel approach that solves  simultaneously for both camera orientations and positions. We offer a complete characterization of the algebraic conditions that enable a unique Euclidean reconstruction of $n$ cameras from a collection of $(^n_2)$ essential matrices. We next use these conditions to formulate essential matrix averaging as a constrained optimization problem, allowing us to recover a consistent set of essential matrices given a (possibly partial) set of measured essential matrices computed independently for pairs of images. We finally use the recovered essential matrices to determine the global positions and orientations of the $n$ cameras. We test our method on common SfM datasets, demonstrating high accuracy while maintaining efficiency and robustness, compared to existing methods.
">Abstract</a>
<a href="https://arxiv.org/pdf/1904.02663.pdf" target="_blank" class="btn btn-default abstract" ptitle="Algebraic Characterization of Essential Matrices and Their Averaging
in Multiview Settings">Paper</a>
<a href="https://github.com/amnonge/Multi-view-Essential-Matrix" class="btn btn-default" target="_blank">Code</a>
<a href="https://www.youtube.com/watch?v=P1stdNCttZY" class="btn btn-default" target="_blank">Video Lecture (Israel Computer Vision Day 2019) </a>
<!--<a href="https://github.com/YuvalBahat/Confidence_From_Invariance" class="btn btn-default">Code</a>-->
</div>
        </div>



<div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/cvpr2019.PNG" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>GPSfM: Global Projective SFM Using Algebraic Constraints</h2>
<h2>on Multi-View Fundamental Matrices</h2>
<p>Yoni Kasten*, Amnon Geifman*, Meirav Galun and  Ronen Basri  (*equal contribution)<p>
<tab1>  </tab1>
<p><i>Computer Vision and Pattern Recognition (CVPR'19), Long Beach 2019</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View Fundamental Matrices"
abstract="This paper addresses the problem of recovering projective camera matrices from collections of fundamental matrices in multiview settings. We make two main contributions. First, given ${n \choose 2}$ fundamental matrices computed for $n$ images, we provide a complete algebraic characterization in the form of conditions that are both necessary and sufficient to enabling the recovery of camera matrices. These conditions are based on arranging the fundamental matrices as blocks in a single matrix, called the $n$-view fundamental matrix, and characterizing this matrix in terms of the signs of its eigenvalues and rank structures. Secondly, we propose a concrete algorithm for projective structure-from-motion that utilizes this characterization. Given a complete or  partial collection of measured fundamental matrices,  our method seeks camera matrices that minimize a global algebraic error for the measured fundamental matrices. In contrast to existing methods, our optimization, without any initialization,  produces a consistent set of fundamental matrices that corresponds to a unique set of cameras (up to a choice of projective frame).  Our experiments indicate that our method achieves state of the art performance in both accuracy and running time.          
">Abstract</a>
<a href="https://arxiv.org/pdf/1812.00426.pdf" target="_blank" class="btn btn-default abstract" ptitle="GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View Fundamental Matrices">Paper</a>
<a href="https://github.com/amnonge/GPSFM-code" class="btn btn-default" target="_blank">Code</a>
<a href="https://www.youtube.com/watch?v=P1stdNCttZY" class="btn btn-default" target="_blank">Video Lecture (Israel Computer Vision Day 2019) </a>
</div>
        </div>


<div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/wacv2019.PNG" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>Resultant Based Incremental Recovery of Camera Pose </h2>
<h2>from Pairwise Matches</h2>
<p>Yoni Kasten, Meirav Galun and  Ronen Basri  <p>
<tab1>  </tab1>
<p><i>Winter Conference on Applications of Computer Vision (WACV'19), Hilton Waikoloa Village, Hawaii 2019</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="Resultant Based Incremental Recovery of Camera Pose from Pairwise Matches"
abstract="Incremental (online) structure from motion pipelines seek to recover the camera matrix associated with an image I_n given n-1 images, I_1,...,I_n-1, whose camera matrices have already been recovered. In this paper, we introduce a novel solution to the six-point online algorithm to recover the exterior parameters associated with I_n. Our algorithm uses just six corresponding pairs of 2D points, extracted each from I_n and from any of the preceding n-1 images, allowing the recovery of the full six degrees of freedom of the n'th camera, and unlike common methods, does not require tracking feature points in three or more images. Our novel solution is based on constructing a Dixon resultant, yielding a solution method that is both efficient and accurate compared to existing solutions. We further use Bernstein's theorem to prove a tight bound on the number of complex solutions. Our experiments demonstrate the utility of our approach.        
">Abstract</a>
<a href="https://arxiv.org/pdf/1901.09364.pdf"  target="_blank" class="btn btn-default abstract" ptitle="Resultant Based Incremental Recovery of Camera Pose from Pairwise Matches">Paper</a>
<a href="https://github.com/ykasten/resultantCamPose" class="btn btn-default" target="_blank">Code</a>
<a href="https://www.youtube.com/watch?v=B_NzjQFZUN4" class="btn btn-default" target="_blank">Video Lecture (Israel Computer Vision Day 2018)</a>
</div>
        </div>

<div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/icip2018.png" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>Two View Constraints on the Epipoles from Few Correspondences </h2>
<p>Yoni Kasten, Michael Werman  <p>
<tab1>  </tab1>
<p><i>International Conference on Image Processing (ICIP'18), Athens 2018</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="Two View Constraints on the Epipoles from Few Correspondences"
abstract="In general it requires at least 7 point correspondences to compute the fundamental matrix between views. We use the cross ratio invariance between corresponding epipolar lines, stemming from epipolar line homography, to derive a simple formulation for the relationship between epipoles and corresponding points. We show how it can be used to reduce the number of required points for the epipolar geometry when some information about the epipoles is available and demonstrate this with a buddy search app.  
">Abstract</a>
<a href="https://arxiv.org/pdf/1810.09496.pdf" target="_blank" class="btn btn-default abstract" ptitle="Two View Constraints on the Epipoles from Few Correspondences">Paper</a>
<a href="buddy_search.html" class="btn btn-default" target="_blank">Buddy Search Web App (developed with Tomer Hacohen)</a>
</div>
        </div>

<div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/eccv2016.gif" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>Fundamental Matrices from Moving Objects Using Line Motion Barcodes </h2>
<p>Yoni Kasten, Gil Ben-Artzi, Shmuel Peleg and Michael Werman  <p>
<tab1>  </tab1>
<p><i>European Conference on Computer Vision (ECCV'16), Amsterdam 2016</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="Fundamental Matrices from Moving Objects Using Line Motion Barcodes"
abstract="Computing the epipolar geometry between cameras with very different viewpoints is often very difficult. The appearance of objects can vary greatly, and it is difficult to find corresponding feature points. Prior methods searched for corresponding epipolar lines using points on the convex hull of the silhouette of a single moving object. These methods fail when the scene includes multiple moving objects. This paper extends previous work to scenes having multiple moving objects by using the ?otion Barcodes? a temporal signature of lines. Corresponding epipolar lines have similar motion barcodes, and candidate pairs of corresponding epipoar lines are found by the similarity of their motion barcodes. As in previous methods we assume that cameras are relatively stationary and that moving objects have already been extracted using background subtraction.  
">Abstract</a>
<a href="https://arxiv.org/pdf/1607.07660.pdf" target="_blank" class="btn btn-default abstract" ptitle="Fundamental Matrices from Moving Objects Using Line Motion Barcodes">Paper</a>
</div>
        </div>

<div class="row" id="templatemo_publications_LargeScaleBD">
<div class="col-md-1"></div>
<div class="col-md-5 col-sm-7 col-xs-24">
<img src="images/cvpr2016.png" alt="image 1">
</div>
<div class="col-md-1"></div>
<div class="col-md-16">
<h2>Camera Calibration From Dynamic Silhouettes Using Motion Barcodes </h2>
<p>Gil Ben-Artzi,Yoni Kasten, Shmuel Peleg and Michael Werman  <p>
<tab1>  </tab1>
<p><i>Computer Vision and Pattern Recognition (CVPR'16), Las Vegas 2016</i></p>
</p>
<!--<a class="btn btn-default abstract" ptitle="Abstract will be available soon...">Abstract</a>-->
<a class="btn btn-default abstract" ptitle="Camera Calibration From Dynamic Silhouettes Using Motion Barcodes"
abstract="Computing the epipolar geometry between cameras with very different viewpoints is often problematic as matching points are hard to find. In these cases, it has been proposed to use information from dynamic objects in the scene for suggesting point and line correspondences. We propose a speed up of about two orders of magnitude, as well as an increase in robustness and accuracy, to methods computing epipolar geometry from dynamic silhouettes based on a new temporal signature, motion barcode for lines. This is a binary temporal sequence for lines, indicating for each frame the existence of at least one foreground pixel on that line. The motion barcodes of two corresponding epipolar lines are very similar so the search for corresponding epipolar lines can be limited to lines having similar barcodes leading to increased speed, accuracy, and robustness in computing the epipolar geometry.  
">Abstract</a>
<a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Ben-Artzi_Camera_Calibration_From_CVPR_2016_paper.pdf" target="_blank" class="btn btn-default abstract" ptitle="Camera Calibration From Dynamic Silhouettes Using Motion Barcodes">Paper</a>
</div>
        </div>

</div></section><!-- end of templatemo_publications -->

<br>
<br>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-76024733-2', 'auto');
  ga('send', 'pageview');

</script>
<div id="lightbox" style="display:none;"><a href="#" class="lightbox-close lightbox-button"></a><div class="lightbox-nav" style="display: none;"><a href="#" class="lightbox-previous lightbox-button"></a><a href="#" class="lightbox-next lightbox-button"></a></div><div href="#" class="lightbox-caption"><p></p></div></div></body><!-- Mirrored from www.wisdom.weizmann.ac.il/~shaharko/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 06 Apr 2016 11:56:56 GMT --></html><script id="f5_cspm">(function(){var f5_cspm={f5_p:'IOHAGAJCCJLFEHNFGLFAFJIDEHNLBIMHHIENOOIFILJIGLNIECMNPHFJBEPONDKELFKPFDBGPNABIAMBMBFBJOFECDNAOPGEAACPPLEJHOHNIKJKAABBIILKKENCEOAF',setCharAt:function(str,index,chr){if(index>str.length-1)return str;return str.substr(0,index)+chr+str.substr(index+1);},get_byte:function(str,i){var s=(i/16)|0;i=(i&15);s=s*32;return((str.charCodeAt(i+16+s)-65)<<4)|(str.charCodeAt(i+s)-65);},set_byte:function(str,i,b){var s=(i/16)|0;i=(i&15);s=s*32;str=f5_cspm.setCharAt(str,(i+16+s),String.fromCharCode((b>>4)+65));str=f5_cspm.setCharAt(str,(i+s),String.fromCharCode((b&15)+65));return str;},set_latency:function(str,latency){latency=latency&0xffff;str=f5_cspm.set_byte(str,48,(latency>>8));str=f5_cspm.set_byte(str,49,(latency&0xff));str=f5_cspm.set_byte(str,43,2);return str;},wait_perf_data:function(){try{var wp=window.performance.timing;if(wp.loadEventEnd>0){var res=wp.loadEventEnd-wp.navigationStart;if(res<60001){var cookie_val=f5_cspm.set_latency(f5_cspm.f5_p,res);window.document.cookie='f5avr0600570113aaaaaaaaaaaaaaaa='+encodeURIComponent(cookie_val)+';path=/';}
return;}}
catch(err){return;}
setTimeout(f5_cspm.wait_perf_data,100);return;},go:function(){var chunk=window.document.cookie.split(/\s*;\s*/);for(var i=0;i<chunk.length;++i){var pair=chunk[i].split(/\s*=\s*/);if(pair[0]=='f5_cspm'&&pair[1]=='1234')
{var d=new Date();d.setTime(d.getTime()-1000);window.document.cookie='f5_cspm=;expires='+d.toUTCString()+';path=/;';setTimeout(f5_cspm.wait_perf_data,100);}}}}
f5_cspm.go();}());</script>